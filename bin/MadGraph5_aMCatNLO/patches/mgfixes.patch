diff -ur MG5_aMC_v2_4_0/madgraph/interface/amcatnlo_run_interface.py ../new/MG5_aMC_v2_4_0/madgraph/interface/amcatnlo_run_interface.py
--- MG5_aMC_v2_4_0/madgraph/interface/amcatnlo_run_interface.py	2016-05-12 13:08:53.000000001 +0200
+++ ../new/MG5_aMC_v2_4_0/madgraph/interface/amcatnlo_run_interface.py	2016-05-19 07:58:49.000000001 +0200
@@ -961,7 +961,7 @@
         self.check_shower(argss, options)
         evt_file = pjoin(os.getcwd(), argss[0], 'events.lhe')
         self.ask_run_configuration('onlyshower', options)
-        self.run_mcatnlo(evt_file)
+        self.run_mcatnlo(evt_file, options)
 
         self.update_status('', level='all', update_results=True)
 
@@ -1217,7 +1217,7 @@
         
         if not mode in ['LO', 'NLO', 'noshower', 'noshowerLO'] \
                                                       and not options['parton']:
-            self.run_mcatnlo(evt_file)
+            self.run_mcatnlo(evt_file, options)
         elif mode == 'noshower':
             logger.warning("""You have chosen not to run a parton shower. NLO events without showering are NOT physical.
 Please, shower the Les Houches events before using them for physics analyses.""")
@@ -2774,7 +2774,7 @@
             scale_pdf_info = self.run_reweight(options['reweightonly'])
         self.update_status('Collecting events', level='parton', update_results=True)
         misc.compile(['collect_events'], 
-                    cwd=pjoin(self.me_dir, 'SubProcesses'))
+                    cwd=pjoin(self.me_dir, 'SubProcesses'), nocompile=options['nocompile'])
         p = misc.Popen(['./collect_events'], cwd=pjoin(self.me_dir, 'SubProcesses'),
                 stdin=subprocess.PIPE, 
                 stdout=open(pjoin(self.me_dir, 'collect_events.log'), 'w'))
@@ -2805,7 +2805,7 @@
         return evt_file[:-3]
 
 
-    def run_mcatnlo(self, evt_file):
+    def run_mcatnlo(self, evt_file, options):
         """runs mcatnlo on the generated event file, to produce showered-events
         """
         logger.info('Preparing MCatNLO run')
@@ -2961,7 +2961,7 @@
             #clean the old files
             files.rm([f for f in event_files if 'events.lhe' not in f])
             if self.shower_card['nsplit_jobs'] > 1:
-                misc.compile(['split_events'], cwd = pjoin(self.me_dir, 'Utilities'))
+                misc.compile(['split_events'], cwd = pjoin(self.me_dir, 'Utilities'), nocompile=options['nocompile'])
                 p = misc.Popen([pjoin(self.me_dir, 'Utilities', 'split_events')],
                                 stdin=subprocess.PIPE,
                                 stdout=open(pjoin(self.me_dir, 'Events', self.run_name, 'split_events.log'), 'w'),
diff -ur MG5_aMC_v2_4_0/madgraph/iolibs/export_v4.py ../new/MG5_aMC_v2_4_0/madgraph/iolibs/export_v4.py
--- MG5_aMC_v2_4_0/madgraph/iolibs/export_v4.py	2016-05-12 13:08:53.000000001 +0200
+++ ../new/MG5_aMC_v2_4_0/madgraph/iolibs/export_v4.py	2016-05-17 03:42:51.000000001 +0200
@@ -6267,8 +6267,9 @@
     # An installation is required then, but only if the specified path is the
     # default local one and that the Ninja library appears missing.
     if requires_ninja and (not opt['ninja'] is None) and\
-            os.path.abspath(opt['ninja'])==pjoin(MG5DIR,'HEPTools','lib') and\
-            not os.path.isfile(pjoin(MG5DIR,'HEPTools','lib','libninja.a')):
+            not os.path.isfile(pjoin(os.path.abspath(opt['ninja']),'libninja.a')):
+            #os.path.abspath(opt['ninja'])==pjoin(MG5DIR,'HEPTools','lib') and\
+            #not os.path.isfile(pjoin(MG5DIR,'HEPTools','lib','libninja.a')):
                 # Then install Ninja here from the tarballs in the vendor
                 # directory so that it would work offline too.
                 logger.info(
diff -ur MG5_aMC_v2_4_0/madgraph/various/cluster.py ../new/MG5_aMC_v2_4_0/madgraph/various/cluster.py
--- MG5_aMC_v2_4_0/madgraph/various/cluster.py	2016-05-12 13:08:53.000000001 +0200
+++ ../new/MG5_aMC_v2_4_0/madgraph/various/cluster.py	2016-05-16 08:52:34.000000001 +0200
@@ -19,6 +19,11 @@
 import glob
 import inspect
 import sys
+import platform
+import signal
+import uuid
+import socket
+import atexit
 
 logger = logging.getLogger('madgraph.cluster') 
 
@@ -83,6 +88,9 @@
     else:
         return True
 
+def cleansubproc(subproc):
+    subproc.terminate()
+
 class Cluster(object):
     """Basic Class for all cluster type submission"""
     name = 'mother class'
@@ -97,6 +105,7 @@
         self.submitted_dirs = [] #HTCaaS
         self.submitted_exes = [] #HTCaaS
         self.submitted_args = [] #HTCaaS
+        self.hold_msg = ""
 
         if 'cluster_queue' in opts:
             self.cluster_queue = opts['cluster_queue']
@@ -307,7 +316,8 @@
             else:
                 nb_job = idle + run + finish + fail
             if fail:
-                raise ClusterManagmentError('Some Jobs are in a Hold/... state. Please try to investigate or contact the IT team')
+                raise ClusterManagmentError('Some Jobs are in a Hold/... state. Error messages are below.' 
+                        'Please try to investigate or contact the IT team. \n%s' % self.hold_msg)
             if idle + run == 0:
                 #time.sleep(20) #security to ensure that the file are really written on the disk
                 logger.info('All jobs finished')
@@ -852,6 +862,7 @@
                   Universe = vanilla
                   notification = Error
                   Initialdir = %(cwd)s
+                  Request_memory = 528
                   %(requirement)s
                   getenv=True
                   queue 1
@@ -928,6 +939,7 @@
                   Universe = vanilla
                   notification = Error
                   Initialdir = %(cwd)s
+                  Request_memory = 528
                   %(requirement)s
                   getenv=True
                   queue 1
@@ -1035,6 +1047,13 @@
                     idle += 1
                 elif status == 'R':
                     run += 1
+                elif status == 'H':
+                    error = misc.Popen(["condor_q", "-format", "'%s\n'", "HoldReason", id], 
+                                            stdout=subprocess.PIPE)
+                    self.hold_msg += "Hold message for ID %s:" % id
+                    for line in error.stdout:
+                        self.hold_msg += line
+                    fail += 1                    
                 elif status != 'C':
                     fail += 1
 
@@ -1361,6 +1380,80 @@
     name = 'lsf'
     job_id = 'LSB_JOBID'
 
+    def __init__(self,*args, **opts):
+        """Init the cluster"""
+        Cluster.__init__(self,*args, **opts)
+
+        if self.temp_dir!=None:
+            self.dorsync = True
+            #print "starting rsync"
+          
+            cwd = os.getcwd()
+
+            self.rsyncroot = cwd
+            
+            self.rsyncmodule = str(uuid.uuid4())
+            
+            #get free port number for rsyncd
+            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
+            sock.bind(('localhost', 0))
+            addr, port = sock.getsockname()
+            sock.close()    
+            
+            self.rsyncport = port
+            #print self.rsyncport
+            
+            rsynclog = os.path.join(cwd, 'rsyncd_%i.log' % self.rsyncport)
+            rsynclock = os.path.join(cwd, 'rsyncd_%i.lock' % self.rsyncport)
+            rsyncpid = os.path.join(cwd, 'rsyncd_%i.pid' % self.rsyncport)
+
+            rsyncpasswd = str(uuid.uuid4())
+            
+            self.rsyncuser = 'madgraph'
+            
+            rsyncsecrets = "%s:%s" % (self.rsyncuser,rsyncpasswd)
+            rsyncsecretsfile = os.path.join(cwd, 'rsyncsecrets_%i' % self.rsyncport)
+            secretsh = open(rsyncsecretsfile,'w')
+            os.chmod(rsyncsecretsfile, 0600)
+            secretsh.write(rsyncsecrets)
+          
+            os.environ["MADGRAPHRSYNCPASSWD_%i" % self.rsyncport] = rsyncpasswd
+            #print rsyncpasswd
+
+            rsyncconf = """
+              port = %(rsyncport)s
+              pid file = %(rsyncpid)s
+              log file = %(rsynclog)s
+             
+                [%(rsyncmodule)s]
+                  comment = Random things available for download
+                  lock file = %(rsynclock)s
+                  secrets file = %(rsyncsecrets)s
+                  path = %(path)s
+                  list = yes 
+                  use chroot = no
+                  munge symlinks = no
+                  read only = no
+                  auth users = %(rsyncuser)s
+            """ % {'rsyncport': self.rsyncport,
+                   'rsyncmodule': self.rsyncmodule,
+                   'path': cwd,
+                  'rsynclog' : rsynclog,
+                  'rsynclock' : rsynclock,
+                  'rsyncpid' : rsyncpid,
+                  'rsyncsecrets' : rsyncsecretsfile,
+                  'rsyncuser' : self.rsyncuser,
+                  }
+            
+            rsyncconffile = os.path.join(cwd, 'rsyncd_%i.conf' % self.rsyncport)
+            open(rsyncconffile,'w').write(rsyncconf)
+            
+            self.rsyncd = subprocess.Popen(['rsync','--daemon', '--no-detach', '--config=%s' % rsyncconffile],cwd=cwd,stdout=subprocess.PIPE,stdin=subprocess.PIPE,stderr=subprocess.PIPE)
+            atexit.register(cleansubproc,self.rsyncd)
+            
+        else:
+            self.dorsync = False
+
     @multiple_try()
     def submit(self, prog, argument=[], cwd=None, stdout=None, stderr=None, log=None,
                required_output=[], nb_submit=0):
@@ -1384,6 +1477,8 @@
         if log is None:
             log = '/dev/null'
         
+        text += 'if [ -n $CMSSW_BASE ]; then cd $CMSSW_BASE; eval `scramv1 runtime -sh`; cd -; fi;'
+        
         text += prog
         if argument:
             text += ' ' + ' '.join(argument)
@@ -1410,6 +1505,137 @@
         return id        
         
         
+    @store_input()
+    @multiple_try()
+    def submit2(self, prog, argument=[], cwd=None, stdout=None, stderr=None,
+            log=None, input_files=[], output_files=[], required_output=[],nb_submit=0):
+        """How to make one submission. Return status id on the cluster.
+        NO SHARE DISK"""
+
+        #print "running lsf submit2"
+
+        if cwd is None:
+            cwd = os.getcwd()
+        if not os.path.exists(prog):
+            prog = os.path.join(cwd, prog)
+
+        if not required_output and output_files:
+            required_output = output_files
+
+        if not self.dorsync or (not input_files and not output_files):
+            # not input/output so not using submit2
+            return self.submit(prog, argument, cwd, stdout, stderr, log,
+        required_output=required_output, nb_submit=nb_submit)
+
+        if self.rsyncd.poll()!=None:
+            raise RuntimeError("rsyncd not running")
+
+        if cwd is None:
+            cwd = os.getcwd()
+        if not os.path.exists(prog):
+            prog = os.path.join(cwd, prog)
+        temp_file_name = "sub." + os.path.basename(prog) + '.'.join(argument)
+               
+        input_files.append(prog)                
+               
+        hostname = platform.node()
+               
+        rsynccwd = cwd
+        if rsynccwd.startswith(self.rsyncroot):
+            rsynccwd = rsynccwd[len(self.rsyncroot):]                   
+               
+        infilelist = ""
+        for input_file in input_files:
+            #make sure input_files are absolute paths
+            if not input_file.startswith('/'):
+                input_file = os.path.join(cwd,input_file)
+            #convert to paths relative to rsyncd root
+            if input_file.startswith(self.rsyncroot):
+                input_file = input_file[len(self.rsyncroot):]
+            infilelist += "%s@%s::%s/%s " % (self.rsyncuser,hostname,self.rsyncmodule, input_file)
+        infilelist += "./"
+        
+        outfilelist = ""
+        for output_file in output_files:
+            outfilelist += "%s " % (output_file)
+        outfilelist += "%s@%s::%s/%s" % (self.rsyncuser,hostname,self.rsyncmodule,rsynccwd)
+            
+        text = """#!/bin/bash
+        
+            SUBMITTERHOST=%(hostname)s            
+
+            if [ -n $CMSSW_VERSION ]
+            then
+              scramv1 project CMSSW $CMSSW_VERSION
+              cd $CMSSW_VERSION
+              eval `scramv1 runtime -sh`
+              cd -
+            fi
+                             
+            export RSYNC_PASSWORD=$MADGRAPHRSYNCPASSWD_%(rsyncport)s
+                 
+            #dereference symlinks for input
+            rsync -vvv --timeout=600 --contimeout=600 --port %(rsyncport)s -rptL %(infilelist)s
+
+            echo '%(arguments)s' > arguments
+            chmod +x ./%(script)s        
+            %(program)s ./%(script)s %(arguments)s
+            
+            #copy symlinks as symlinks for output and don't overwrite existing files unless updated
+            rsync -vvv --timeout=600 --contimeout=600 --port %(rsyncport)s -rptul %(outfilelist)s
+            
+            """
+        dico = {'script': os.path.basename(prog),
+        'hostname': hostname,
+        'infilelist': infilelist,
+        'outfilelist': outfilelist,
+        'output_files': ' '.join(output_files),
+        'rsyncport': self.rsyncport,
+        'arguments': ' '.join([str(a) for a in argument]),
+        'program': ' ' if '.py' in prog else 'bash'}
+
+        me_dir = self.get_jobs_identifier(cwd, prog)
+
+        text = text % dico
+        cwdpath = "/tmp/" + os.environ.get("USER", '')
+        command = ['bsub', '-cwd', cwdpath, '-C0', '-J', me_dir]
+        if cwd is None:
+            cwd = os.getcwd()
+        #else:
+            #text += " cd %s;" % cwd
+        if stdout and isinstance(stdout, str):
+            command.extend(['-o', stdout])
+        if stderr and isinstance(stdout, str):
+            command.extend(['-e', stderr])
+        elif stderr == -2: # -2 is subprocess.STDOUT
+            pass
+        if log is None:
+            log = '/dev/null'
+
+        if self.cluster_queue and self.cluster_queue != 'None':
+            command.extend(['-q', self.cluster_queue])
+
+        submitenv = os.environ.copy()
+        submitenv["TMPDIR"] = "/tmp/" + submitenv.get("USER", '')
+        a = misc.Popen(command, stdout=subprocess.PIPE,
+        stderr=subprocess.STDOUT,
+        stdin=subprocess.PIPE, cwd=cwd,
+        env=submitenv)
+
+        output = a.communicate(text)[0]
+        #Job <nnnn> is submitted to default queue <normal>.
+        try:
+            id = output.split('>',1)[0].split('<')[1]
+        except:
+            raise ClusterManagmentError, 'fail to submit to the cluster: \n%s' \
+            % output
+        if not id.isdigit():
+            raise ClusterManagmentError, 'fail to submit to the cluster: \n%s' \
+            % output
+        self.submitted += 1
+        self.submitted_ids.append(id)
+        return id         
+        
     @multiple_try()
     def control_one_job(self, id):
         """ control the status of a single job with it's cluster id """
@@ -1439,38 +1665,49 @@
         """ control the status of a single job with it's cluster id """
         
         if not self.submitted_ids:
-            return 0, 0, 0, 0
-        
-        cmd = "bjobs " + ' '.join(self.submitted_ids) 
-        status = misc.Popen([cmd], shell=True, stdout=subprocess.PIPE)
+            return 0, 0, 0, 0        
 
         jobstatus = {}
-        for line in status.stdout:
-            line = line.strip()
-            if 'JOBID' in line:
-                continue
-            splitline = line.split()
-            id = splitline[0]
-            if id not in self.submitted_ids:
-                continue
-            jobstatus[id] = splitline[2]
+        
+        #split into smaller groups of 200 jobs to avoid problems with truncated output
+        idsplitting = 200
+        splitids = [self.submitted_ids[i:i+idsplitting] for i in range(0, len(self.submitted_ids), idsplitting)]
+        
+        for ids in splitids:
+            cmd = "bjobs " + ' '.join(ids) 
+            status = misc.Popen([cmd], shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
+
+            for line in status.stdout:
+                line = line.strip()
+                if 'JOBID' in line:
+                    continue
+                splitline = line.split()
+                if splitline[0] in ids:
+                    id = splitline[0]
+                    jobstatus[id] = splitline[2]
+                else:
+                    splitline = re.split('[><]',line)
+                    if len(splitline)==3 and splitline[0] == 'Job ' and splitline[2] == " is not found" and splitline[1] in ids:
+                        id = splitline[1]
+                        jobstatus[id] = 'MISSING'        
 
         idle, run, fail = 0, 0, 0
         for id in self.submitted_ids[:]:
             if id in jobstatus:
                 status = jobstatus[id]
             else:
-                status = 'MISSING'
+                status = 'PEND'
+
             if status == 'RUN':
                 run += 1
-            elif status == 'PEND':
-                idle += 1
-            else:
+            elif status in ['DONE', 'EXIT', 'MISSING']:
                 status = self.check_termination(id)
                 if status == 'wait':
                     run += 1
                 elif status == 'resubmit':
                     idle += 1                
+            else:
+                idle += 1
 
         return idle, run, self.submitted - (idle+run+fail), fail
 
diff -ur MG5_aMC_v2_4_0/madgraph/various/misc.py ../new/MG5_aMC_v2_4_0/madgraph/various/misc.py
--- MG5_aMC_v2_4_0/madgraph/various/misc.py	2016-05-12 13:08:53.000000001 +0200
+++ ../new/MG5_aMC_v2_4_0/madgraph/various/misc.py	2016-05-19 07:55:38.000000001 +0200
@@ -455,6 +455,18 @@
 def compile(arg=[], cwd=None, mode='fortran', job_specs = True, nb_core=1 ,**opt):
     """compile a given directory"""
 
+    if 'nocompile' in opt:
+        if opt['nocompile'] == True:
+            if not arg:
+                return
+            if cwd:
+                executable = pjoin(cwd, arg[0])
+            else:
+                executable = arg[0]
+            if os.path.exists(executable):
+                return
+        del opt['nocompile']
+
     cmd = ['make']
     try:
         if nb_core > 1:
diff -ur MG5_aMC_v2_4_0/MadSpin/interface_madspin.py ../new/MG5_aMC_v2_4_0/MadSpin/interface_madspin.py
--- MG5_aMC_v2_4_0/MadSpin/interface_madspin.py	2016-05-12 13:08:53.000000001 +0200
+++ ../new/MG5_aMC_v2_4_0/MadSpin/interface_madspin.py	2016-05-19 10:26:15.000000001 +0200
@@ -464,7 +464,7 @@
         except:
             #cleaning if the error is recover later
             key = line.split()[0]
-            if hasattr(self, 'multiparticles_ms' and key in self.multiparticles_ms):
+            if hasattr(self, 'multiparticles_ms') and key in self.multiparticles_ms:
                 del self.multiparticles_ms[key]
             raise
            
diff -ur MG5_aMC_v2_4_0/Template/LO/bin/internal/Gridpack/run.sh ../new/MG5_aMC_v2_4_0/Template/LO/bin/internal/Gridpack/run.sh
--- MG5_aMC_v2_4_0/Template/LO/bin/internal/Gridpack/run.sh	2016-05-12 13:08:53.000000001 +0200
+++ ../new/MG5_aMC_v2_4_0/Template/LO/bin/internal/Gridpack/run.sh	2016-05-16 07:47:18.000000001 +0200
@@ -78,43 +78,5 @@
     cd ..
 fi
 
-if [[ -e ./DECAY/decay ]]; then
-    cd DECAY
-    echo -$seed > iseed.dat
-    for ((i = 1 ;  i <= 20;  i++)) ; do
-	if [[ -e decay_$i\.in ]]; then
-	    echo "Decaying events..."
-	    mv ../events.lhe ../events_in.lhe
-	    ./decay < decay_$i\.in
-	fi
-    done
-    cd ..
-fi
-
-if [[ -e ./REPLACE/replace.pl ]]; then
-    for ((i = 1 ;  i <= 20;  i++)) ; do
-	if [[ -e ./REPLACE/replace_card$i\.dat ]];then
-	    echo "Adding flavors..."
-	    mv ./events.lhe ./events_in.lhe
-	    cd ./REPLACE
-	    ./replace.pl ../events_in.lhe ../events.lhe < replace_card$i\.dat
-	    cd ..
-	fi
-    done
-fi
-
-# part added by Stephen Mrenna to correct the kinematics of the replaced
-#  particles
-if [[ -e ./madevent/bin/internal/addmasses.py ]]; then
-  mv ./events.lhe ./events.lhe.0
-  python ./madevent/bin/internal/addmasses.py ./events.lhe.0 ./events.lhe
-  if [[ $? -eq 0 ]]; then
-     echo "Mass added"
-     rm -rf ./events.lhe.0 &> /dev/null
-  else
-     mv ./events.lhe.0 ./events.lhe
-  fi
-fi  
-
 gzip -f events.lhe
 exit
diff -ur MG5_aMC_v2_4_0/Template/NLO/SubProcesses/MCmasses_PYTHIA8.inc ../new/MG5_aMC_v2_4_0/Template/NLO/SubProcesses/MCmasses_PYTHIA8.inc
--- MG5_aMC_v2_4_0/Template/NLO/SubProcesses/MCmasses_PYTHIA8.inc	2016-05-12 13:08:53.000000001 +0200
+++ ../new/MG5_aMC_v2_4_0/Template/NLO/SubProcesses/MCmasses_PYTHIA8.inc	2016-05-16 07:23:59.000000001 +0200
@@ -1,12 +1,12 @@
-      mcmass(1)=0.33d0
-      mcmass(2)=0.33d0
-      mcmass(3)=0.50d0
-      mcmass(4)=1.50d0
-      mcmass(5)=4.80d0
-      mcmass(11)=0.510998928d-3
-      mcmass(12)=0.d0
-      mcmass(13)=0.1056583715d0
-      mcmass(14)=0.d0
-      mcmass(15)=1.77682d0
-      mcmass(16)=0.d0
+      mcmass(1)=0.0d0
+      mcmass(2)=0.0d0
+      mcmass(3)=0.0d0
+      mcmass(4)=0.0d0
+      mcmass(5)=0.0d0
+      mcmass(11)=0.0d0
+      mcmass(12)=0.0d0
+      mcmass(13)=0.0d0
+      mcmass(14)=0.0d0
+      mcmass(15)=0.0d0
+      mcmass(16)=0.0d0
       mcmass(21)=0.0d0
